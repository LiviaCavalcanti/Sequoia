{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(nn.Module):\n",
    "    '''\n",
    "    Object wrapper class.\n",
    "    This a wrapper for objects. It is initialiesed with the object to wrap\n",
    "    and then proxies the unhandled getattribute methods to it.\n",
    "    Other classes are to inherit from it.\n",
    "    '''\n",
    "    def __init__(self, obj):\n",
    "        '''\n",
    "        Wrapper constructor.\n",
    "        @param obj: object to wrap\n",
    "        '''\n",
    "        # wrap the object\n",
    "        super(Wrapper, self).__init__()\n",
    "        self.__class__ = type(obj)\n",
    "        self._wrapped_obj = obj\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        # see if this object has attr\n",
    "        # NOTE do not use hasattr, it goes into\n",
    "        # infinite recurrsion\n",
    "        if attr in self.__dict__:\n",
    "            # this object has it\n",
    "            return getattr(self, attr)\n",
    "        # proxy to the wrapped object\n",
    "        return getattr(self._wrapped_obj, attr)\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._wrapped_obj.__call__(*args, **kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import the modules from the falr submodule: cannot import name 'HParams' from 'config' (/Users/oleksostapenko/Projects/SSCL/config.py)\n",
      "Make sure to run `git submodule init; git submodule update`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/SSCL/tasks/simclr/simclr_task.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfalr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfalr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimCLRAugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfalr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimCLRLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SSCL/tasks/simclr/falr/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobal_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperimentType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmin_class_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HParams' from 'config' (/Users/oleksostapenko/Projects/SSCL/config.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b410b3ee0fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuxiliaryTask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAuxiliaryTaskOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfix_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SSCL/tasks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpatch_location\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatchLocationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAEReconstructionTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuxiliaryTaskOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransformation_based\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdjustBrightnessTask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRotationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SSCL/tasks/tasks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAEReconstructionTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAEReconstructionTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimclr_task\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimCLRTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m from .transformation_based import (AdjustBrightnessTask,\n\u001b[1;32m     17\u001b[0m                                    \u001b[0mClassifyTransformationTask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SSCL/tasks/simclr/simclr_task.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't import the modules from the falr submodule: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Make sure to run `git submodule init; git submodule update`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import (Any, Dict, List, NamedTuple, Optional, Tuple, Type,\n",
    "                    TypeVar, Union)\n",
    "\n",
    "import torch\n",
    "from simple_parsing import MutableField as mutable_field\n",
    "from simple_parsing import choice, field\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from common.layers import ConvBlock, Flatten\n",
    "from common.losses import LossInfo\n",
    "from common.metrics import accuracy, get_metrics\n",
    "from config import Config\n",
    "from tasks import AuxiliaryTask, AuxiliaryTaskOptions, Tasks\n",
    "from utils.utils import fix_channels\n",
    "\n",
    "logger = logging.getLogger(__file__)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    @dataclass\n",
    "    class HParams:\n",
    "        \"\"\" Set of hyperparameters for the classifier.\n",
    "\n",
    "        We use [simple_parsing](www.github.com/lebrice/simpleparsing) to\n",
    "        generate command-line arguments for each attribute of this class.\n",
    "        \"\"\"\n",
    "        batch_size: int = 128   # Input batch size for training.\n",
    "        epochs: int = 10        # Number of epochs to train.\n",
    "        learning_rate: float = field(default=1e-3, alias=\"-lr\")  # learning rate.\n",
    "\n",
    "        # Dimensions of the hidden state (feature extractor/encoder output).\n",
    "        hidden_size: int = 100\n",
    "\n",
    "        # Prevent gradients of the classifier from backpropagating into the encoder.\n",
    "        detach_classifier: bool = False\n",
    "\n",
    "        # Use an encoder architecture from the torchvision.models package.\n",
    "        encoder_model: Optional[str] = choice({\n",
    "            \"vgg16\": models.vgg16,  # This is the only one tested so far.\n",
    "            \"resnet18\": models.resnet18,\n",
    "            \"resnet34\": models.resnet34,\n",
    "            \"resnet50\": models.resnet50,\n",
    "            \"resnet101\": models.resnet101,\n",
    "            \"resnet152\": models.resnet152,\n",
    "            \"alexnet\": models.alexnet,\n",
    "            # \"squeezenet\": models.squeezenet1_0,  # Not supported yet (weird output shape)\n",
    "            \"densenet\": models.densenet161,\n",
    "            # \"inception\": models.inception_v3,  # Not supported yet (creating model takes forever?)\n",
    "            # \"googlenet\": models.googlenet,  # Not supported yet (creating model takes forever?)\n",
    "            \"shufflenet\": models.shufflenet_v2_x1_0,\n",
    "            \"mobilenet\": models.mobilenet_v2,\n",
    "            \"resnext50_32x4d\": models.resnext50_32x4d,\n",
    "            \"wide_resnet50_2\": models.wide_resnet50_2,\n",
    "            \"mnasnet\": models.mnasnet1_0,\n",
    "        }, default=None)\n",
    "        # Use the pretrained weights of the ImageNet model from torchvision.\n",
    "        pretrained_model: bool = False\n",
    "        # Freeze the weights of the pretrained encoder (except the last layer,\n",
    "        # which projects from their hidden size to ours).\n",
    "        freeze_pretrained_model: bool = False\n",
    "\n",
    "\n",
    "        aux_tasks: AuxiliaryTaskOptions = field(default_factory=AuxiliaryTaskOptions)\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int, ...],\n",
    "                 num_classes: int,\n",
    "                 encoder: nn.Module,\n",
    "                 classifier: nn.Module,\n",
    "                #  auxiliary_task_options: AuxiliaryTaskOptions,\n",
    "                 hparams: HParams,\n",
    "                 config: Config):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        # Feature extractor\n",
    "        self.encoder = encoder\n",
    "        # Classifier output layer\n",
    "        self.classifier = classifier\n",
    "        self.hparams: Classifier.HParams = hparams\n",
    "        self.config = config\n",
    "\n",
    "        self.hidden_size = hparams.hidden_size  \n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.device = self.config.device\n",
    "\n",
    "        # Share the relevant parameters with all the auxiliary tasks.\n",
    "        # We do this by setting class attributes.\n",
    "        AuxiliaryTask.hidden_size   = self.hparams.hidden_size\n",
    "        AuxiliaryTask.input_shape   = self.input_shape\n",
    "        AuxiliaryTask.encoder       = self.encoder\n",
    "        AuxiliaryTask.classifier    = self.classifier\n",
    "        AuxiliaryTask.preprocessing = self.preprocess_inputs\n",
    "        \n",
    "        # Dictionary of auxiliary tasks.\n",
    "        self.tasks: Dict[str, AuxiliaryTask] = self.hparams.aux_tasks.create_tasks(  # type: ignore\n",
    "            input_shape=input_shape,\n",
    "            hidden_size=self.hparams.hidden_size\n",
    "        )\n",
    "\n",
    "        # Current task label. (Optional, as we shouldn't rely on this.)\n",
    "        # TODO: Replace the classifier model with something like CN-DPM or CURL,\n",
    "        # so we can actually do task-free CL.\n",
    "        self._current_task_id: Optional[str] = None\n",
    "        # Dictionary of classifiers to use if we are provided the task-label.\n",
    "        self.task_classifiers: Dict[str, nn.Module] = nn.ModuleDict()  #type: ignore  \n",
    "\n",
    "        if self.config.debug and self.config.verbose:\n",
    "            logger.debug(self)\n",
    "            logger.debug(\"Auxiliary tasks:\")\n",
    "            for task_name, task in self.tasks.items():\n",
    "                logger.debug(f\"{task.name}: {task.coefficient}\")\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)  \n",
    "\n",
    "    def supervised_loss(self, x: Tensor, y: Tensor, h_x: Tensor=None, y_pred: Tensor=None) -> LossInfo:\n",
    "        h_x = self.encode(x) if h_x is None else h_x\n",
    "        y_pred = self.logits(h_x) if y_pred is None else y_pred\n",
    "        y = y.view(-1)\n",
    "        loss = self.classification_loss(y_pred, y)\n",
    "        metrics = get_metrics(x=x, h_x=h_x, y_pred=y_pred, y=y)\n",
    "        loss_info = LossInfo(\n",
    "            name=Tasks.SUPERVISED,\n",
    "            total_loss=loss,\n",
    "            tensors=(dict(x=x, h_x=h_x, y_pred=y_pred, y=y)),\n",
    "        )\n",
    "        loss_info.metrics[Tasks.SUPERVISED] = metrics\n",
    "        return loss_info\n",
    "\n",
    "    def get_loss(self, x: Tensor, y: Tensor=None) -> LossInfo:\n",
    "        total_loss = LossInfo(\"Train\" if self.training else \"Test\")\n",
    "        h_x = self.encode(x)\n",
    "        y_pred = self.logits(h_x)\n",
    "        \n",
    "        total_loss.total_loss = torch.zeros(1, device=self.device)\n",
    "        total_loss.tensors[\"x\"] = x.detach()\n",
    "        total_loss.tensors[\"h_x\"] = h_x.detach()\n",
    "        total_loss.tensors[\"y_pred\"] = y_pred.detach()\n",
    "\n",
    "        if y is not None:\n",
    "            supervised_loss = self.supervised_loss(x=x, y=y, h_x=h_x, y_pred=y_pred)\n",
    "            total_loss += supervised_loss\n",
    "\n",
    "        for task_name, aux_task in self.tasks.items():\n",
    "            if aux_task.enabled:\n",
    "                aux_task_loss = aux_task.get_scaled_loss(x, h_x=h_x, y_pred=y_pred, y=y)\n",
    "                total_loss += aux_task_loss\n",
    "        \n",
    "        if self.config.debug and self.config.verbose:\n",
    "            for name, loss in total_loss.losses.items():\n",
    "                logger.debug(name, loss.total_loss, loss.metrics)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def encode(self, x: Tensor):\n",
    "        x = self.preprocess_inputs(x)\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def preprocess_inputs(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Preprocess the input tensor x before it is passed to the encoder.\n",
    "        \n",
    "        By default this does nothing. When subclassing the Classifier or \n",
    "        switching datasets, you might want to change this behaviour.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - x : Tensor\n",
    "        \n",
    "            a batch of inputs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            The preprocessed inputs.\n",
    "        \"\"\"\n",
    "        return fix_channels(x)\n",
    "\n",
    "    @property\n",
    "    def current_task_id(self) -> Optional[str]:\n",
    "        if self._current_task_id is None:\n",
    "            return None\n",
    "        return self._current_task_id\n",
    "\n",
    "    @current_task_id.setter\n",
    "    def current_task_id(self, value: Optional[Union[int, str]]):\n",
    "        value = str(value) if value is not None else None\n",
    "        self._current_task_id = value\n",
    "        # If there isn't a classifier for this task\n",
    "        if value and value not in self.task_classifiers.keys():\n",
    "            if self.config.debug:\n",
    "                logger.info(f\"Creating a new classifier for taskid {value}.\")\n",
    "            # Create one starting from the \"global\" classifier.\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            self.task_classifiers[value] = classifier\n",
    "            self.optimizer.add_param_group({\"params\": classifier.parameters()})\n",
    "\n",
    "    def logits(self, h_x: Tensor) -> Tensor:\n",
    "        if self.hparams.detach_classifier:\n",
    "            h_x = h_x.detach()\n",
    "\n",
    "        # Use the \"general\" classifier by default.\n",
    "        classifier = self.classifier\n",
    "        # If a task-id is given, use the task-specific classifier.\n",
    "        if self.current_task_id is not None:\n",
    "            classifier = self.task_classifiers[self.current_task_id]\n",
    "        return classifier(h_x)\n",
    "    \n",
    "    def load_state_dict(self, state_dict: Dict) -> Tuple[List[str], List[str]]:\n",
    "        current_task_id = self.current_task_id\n",
    "        for key in state_dict:\n",
    "            if key.startswith(\"task_classifiers\"):\n",
    "                n = key.split(\".\")[1]\n",
    "                self.current_task_id = n\n",
    "        return super().load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C('Geeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Wrapper(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gg\n"
     ]
    }
   ],
   "source": [
    "cc(\"gg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.name = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Other'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cbc72485fff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0e6cb981aad1>\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-8-0e6cb981aad1>\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "c.print_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (falr)",
   "language": "python",
   "name": "pycharm-5e6e0d1b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
